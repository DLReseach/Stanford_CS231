<p><img src="media/image1.jpeg" style="width:9.73125in;height:1.92431in" /></p>
<p>Lecture 3:</p>
<p>Loss functions and</p>
<p>Optimization</p>
<p><img src="media/image2.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 1 11 Jan 2016</p>
<blockquote>
<p><img src="media/image3.jpeg" style="width:8.47222in;height:4.24306in" /></p>
<p>A1 is due Jan 20 (Wednesday). ~9 days left</p>
<p>Warning: Jan 18 (Monday) is Holiday (no class/office hours)</p>
</blockquote>
<p><img src="media/image4.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 2 11 Jan 2016</p>
<p><img src="media/image5.jpeg" style="width:9.70625in;height:0.89583in" />Challenges in Visual Recognition</p>
<blockquote>
<p>Camera pose Illumination Deformation Occlusion</p>
</blockquote>
<p><img src="media/image6.jpeg" style="width:8.80417in;height:1.525in" /></p>
<blockquote>
<p><sub>Background clutter</sub> Intraclass variation</p>
</blockquote>
<p><img src="media/image7.jpeg" style="width:10in;height:2.71042in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 3 11 Jan 2016</p>
<blockquote>
<p><img src="media/image8.jpeg" style="width:9.73403in;height:4.68542in" /> data-driven approach, kNN</p>
<p>the data NN classifier 5-NN classifier</p>
</blockquote>
<p><img src="media/image9.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 4 11 Jan 2016</p>
<blockquote>
<p><img src="media/image10.jpeg" style="width:9.4in;height:2.20278in" /> Linear classifier</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td>image parameters</td>
<td><blockquote>
<p><strong>10</strong> numbers, indicating</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p>f(<strong>x</strong>,<strong>W</strong>)</p>
</blockquote></td>
<td><blockquote>
<p>class scores</p>
</blockquote></td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>[32x32x3]</strong></p>
</blockquote>
<p><img src="media/image11.jpeg" style="width:3.74931in" /></p>
<blockquote>
<p>array of numbers 0...1</p>
</blockquote>
<p><img src="media/image12.jpeg" style="width:2.825in;height:2.07986in" /></p>
<blockquote>
<p>(3072 numbers total)</p>
</blockquote>
<p><img src="media/image13.jpeg" style="width:10in;height:2.55347in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 5 11 Jan 2016</p>
<blockquote>
<p><img src="media/image15.jpeg" style="width:10in;height:5.55972in" />Going forward: Loss function/Optimization</p>
</blockquote>
<p><img src="media/image16.jpeg" style="height:4.00556in" /></p>
<blockquote>
<p>-3.45 -0.51 3.42</p>
<p><sup>-8.87</sup> <strong>6.04</strong> 4.64</p>
<p><sup>0.09</sup> 5.31 2.65</p>
<p><strong><sup>2.9</sup></strong> -4.22 5.1</p>
<p><sup>4.48</sup> -4.19 2.64</p>
<p><sup>8.02</sup> 3.58 5.55</p>
<p><sup>3.78</sup> 4.49 <strong>-4.34</strong></p>
<p><sup>1.06</sup> -4.37 -1.5</p>
<p><sup>-0.36</sup> -2.09 -4.79</p>
<p><sup>-0.72</sup> -2.93 6.14</p>
</blockquote>
<p>TODO:</p>
<ol type="1">
<li><blockquote>
<p>Define a <strong>loss function</strong> that quantifies our unhappiness with the scores across the training data.</p>
</blockquote></li>
<li><blockquote>
<p>Come up with a way of efficiently finding the parameters that minimize the loss function.</p>
</blockquote></li>
</ol>
<blockquote>
<p><strong>(optimization)</strong></p>
</blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 6 11 Jan 2016</p>
<blockquote>
<p><img src="media/image17.jpeg" style="width:8.20139in;height:0.41667in" /></p>
<p>With some W the scores <img src="media/image18.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<p><img src="media/image19.jpeg" style="width:6.10486in;height:1.93056in" /></p>
<blockquote>
<p>cat</p>
</blockquote>
<p><img src="media/image20.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>car</p>
</blockquote>
<p><img src="media/image21.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>frog</p>
<p><strong>3.2</strong> 1.3 2.2</p>
</blockquote>
<p><img src="media/image22.jpeg" style="width:4.43542in;height:0.41667in" /></p>
<blockquote>
<p>5.1 <strong>4.9</strong> 2.5</p>
</blockquote>
<p><img src="media/image23.jpeg" style="width:4.35208in;height:0.41667in" /></p>
<p>-1.7 2.0 <strong>-3.1</strong></p>
<p><img src="media/image24.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 7 11 Jan 2016</p>
<blockquote>
<p><img src="media/image25.jpeg" style="width:9.87083in;height:4.86944in" /></p>
<p>With some W the scores <img src="media/image26.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td>cat</td>
<td><strong>3.2</strong></td>
<td>1.3</td>
<td>2.2</td>
</tr>
<tr class="even">
<td>car</td>
<td>5.1</td>
<td><strong>4.9</strong></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td>frog</td>
<td>-1.7</td>
<td>2.0</td>
<td><strong>-3.1</strong></td>
</tr>
</tbody>
</table>
<p><img src="media/image27.jpeg" style="width:10in;height:0.58889in" /></p>
<p><strong>Multiclass SVM loss:</strong></p>
<p><img src="media/image28.jpeg" style="height:4.85556in" /></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image29.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image30.jpeg" style="width:1.22986in;height:0.23958in" /></p>
</blockquote>
<p>the SVM loss has the form:</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 8 11 Jan 2016</p>
<p><img src="media/image31.jpeg" style="width:9.9in;height:4.86944in" /></p>
<blockquote>
<p>Suppose: 3 training examples, 3 classes.</p>
<p>With some W the scores <img src="media/image32.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td></td>
<td>1.3</td>
<td>2.2</td>
<td></td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td></td>
<td><strong>4.9</strong></td>
<td>2.5</td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>2.9</td>
<td>2.0</td>
<td><strong>-3.1</strong></td>
<td></td>
</tr>
<tr class="even">
<td>Losses:</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image33.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
<p><strong>Multiclass SVM loss:</strong></p>
<p><img src="media/image34.jpeg" style="height:4.85556in" /></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image35.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image36.jpeg" style="width:1.22986in;height:0.23958in" /></p>
</blockquote>
<p>the SVM loss has the form:</p>
<ul>
<li><blockquote>
<p>max(0, 5.1 - 3.2 + 1) +max(0, -1.7 - 3.2 + 1)</p>
</blockquote></li>
<li><blockquote>
<p>max(0, 2.9) + max(0, -3.9)</p>
</blockquote></li>
<li><blockquote>
<p>2.9 + 0</p>
</blockquote></li>
<li><blockquote>
<p>2.9</p>
</blockquote></li>
</ul>
<table>
<tbody>
<tr class="odd">
<td>Lecture 3 9</td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image37.jpeg" style="width:9.9in;height:4.86944in" /></p>
<blockquote>
<p>Suppose: 3 training examples, 3 classes.</p>
<p>With some W the scores <img src="media/image38.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td></td>
<td>2.2</td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td></td>
<td><strong>-3.1</strong></td>
</tr>
<tr class="even">
<td>Losses:</td>
<td>2.9</td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image39.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
<p><strong>Multiclass SVM loss:</strong></p>
<p><img src="media/image40.jpeg" style="height:4.85556in" /></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image41.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image42.jpeg" style="width:1.22986in;height:0.23958in" /></p>
</blockquote>
<p>the SVM loss has the form:</p>
<ul>
<li><blockquote>
<p>max(0, 1.3 - 4.9 + 1) +max(0, 2.0 - 4.9 + 1)</p>
</blockquote></li>
<li><blockquote>
<p>max(0, -2.6) + max(0, -1.9)</p>
</blockquote></li>
<li><blockquote>
<p>0 + 0</p>
</blockquote></li>
<li><blockquote>
<p>0</p>
</blockquote></li>
</ul>
<table>
<tbody>
<tr class="odd">
<td>Lecture 3 10</td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image43.jpeg" style="width:9.9in;height:4.86944in" /></p>
<blockquote>
<p>Suppose: 3 training examples, 3 classes.</p>
<p>With some W the scores <img src="media/image44.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td>1.3</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td><strong>4.9</strong></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td>2.0</td>
<td>10.9</td>
<td></td>
</tr>
<tr class="even">
<td>Losses:</td>
<td>2.9</td>
<td>0</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image45.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
<blockquote>
<p><strong>Multiclass SVM loss:</strong></p>
</blockquote>
<p><img src="media/image46.jpeg" style="height:4.85556in" /></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image47.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image48.jpeg" style="width:1.22986in;height:0.23958in" /></p>
<p>the SVM loss has the form:</p>
</blockquote>
<ul>
<li><blockquote>
<p>max(0, 2.2 - (-3.1) + 1) +max(0, 2.5 - (-3.1) + 1)</p>
</blockquote></li>
<li><blockquote>
<p>max(0, 5.3) + max(0, 5.6)</p>
</blockquote></li>
<li><blockquote>
<p>5.3 + 5.6</p>
</blockquote></li>
<li><blockquote>
<p>10.9</p>
</blockquote></li>
</ul>
<table>
<tbody>
<tr class="odd">
<td>Lecture 3 11</td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p><img src="media/image49.jpeg" style="width:9.9in;height:4.86944in" /></p>
<p>With some W the scores <img src="media/image50.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td>1.3</td>
<td>2.2</td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td><strong>4.9</strong></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td>2.0</td>
<td><strong>-3.1</strong></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Losses:</p>
</blockquote></td>
<td>2.9</td>
<td>0</td>
<td>10.9</td>
</tr>
</tbody>
</table>
<p><img src="media/image51.jpeg" style="width:10in;height:0.58889in" /></p>
<blockquote>
<p><strong>Multiclass SVM loss:</strong></p>
</blockquote>
<p><img src="media/image52.jpeg" style="height:4.85556in" /></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image53.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image54.jpeg" style="width:1.22986in;height:0.23958in" /></p>
<p>the SVM loss has the form:</p>
<p>and the full training loss is the mean over all examples in the training data:</p>
</blockquote>
<p>L = (2.9 + 0 + 10.9)/3</p>
<ul>
<li><blockquote>
<p><strong>4.6</strong></p>
</blockquote></li>
</ul>
<blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 12 11 Jan 2016</p>
<p><img src="media/image55.jpeg" style="height:4.85556in" /></p>
<p>With some W the scores <img src="media/image57.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td>1.3</td>
<td>2.2</td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td><strong>4.9</strong></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td>2.0</td>
<td><strong>-3.1</strong></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Losses:</p>
</blockquote></td>
<td>2.9</td>
<td>0</td>
<td>10.9</td>
</tr>
</tbody>
</table>
<p><img src="media/image58.jpeg" style="width:10in;height:0.58889in" /></p>
<p><strong>Multiclass SVM loss:</strong></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image59.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image60.jpeg" style="width:1.22986in;height:0.23958in" /></p>
<p>the SVM loss has the form:</p>
</blockquote>
<ol start="17" type="A">
<li><blockquote>
<p>what if the sum was instead over all classes?</p>
</blockquote></li>
</ol>
<blockquote>
<p>(including j = y_i)</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 13 11 Jan 2016</p>
<p><img src="media/image61.jpeg" style="height:4.85556in" /></p>
<p>With some W the scores <img src="media/image63.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td>1.3</td>
<td>2.2</td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td><strong>4.9</strong></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td>2.0</td>
<td><strong>-3.1</strong></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Losses:</p>
</blockquote></td>
<td>2.9</td>
<td>0</td>
<td>10.9</td>
</tr>
</tbody>
</table>
<p><img src="media/image64.jpeg" style="width:10in;height:0.58889in" /></p>
<p><strong>Multiclass SVM loss:</strong></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image65.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image66.jpeg" style="width:1.22986in;height:0.23958in" /></p>
<p>the SVM loss has the form:</p>
<p>Q2: what if we used a mean instead of a sum here?</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 14 11 Jan 2016</p>
<p><img src="media/image67.jpeg" style="height:4.85556in" /></p>
<p>With some W the scores <img src="media/image69.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td>1.3</td>
<td>2.2</td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td><strong>4.9</strong></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td>2.0</td>
<td><strong>-3.1</strong></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Losses:</p>
</blockquote></td>
<td>2.9</td>
<td>0</td>
<td>10.9</td>
</tr>
</tbody>
</table>
<p><img src="media/image70.jpeg" style="width:10in;height:0.58889in" /></p>
<p><strong>Multiclass SVM loss:</strong></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image71.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image72.jpeg" style="width:1.22986in;height:0.23958in" /></p>
<p>the SVM loss has the form:</p>
<p>Q3: what if we used</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 15 11 Jan 2016</p>
<p><img src="media/image73.jpeg" style="height:4.85556in" /></p>
<p>With some W the scores <img src="media/image75.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td>1.3</td>
<td>2.2</td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td><strong>4.9</strong></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td>2.0</td>
<td><strong>-3.1</strong></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Losses:</p>
</blockquote></td>
<td>2.9</td>
<td>0</td>
<td>10.9</td>
</tr>
</tbody>
</table>
<p><img src="media/image76.jpeg" style="width:10in;height:0.58889in" /></p>
<p><strong>Multiclass SVM loss:</strong></p>
<blockquote>
<p>Given an example</p>
<p>where is the image and where <img src="media/image77.jpeg" style="width:0.29028in;height:0.29167in" /> is the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image78.jpeg" style="width:1.22986in;height:0.23958in" /></p>
<p>the SVM loss has the form:</p>
<p>Q4: what is the min/max possible loss?</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 16 11 Jan 2016</p>
<p><img src="media/image79.jpeg" style="width:9.9in;height:4.86944in" /></p>
<p>With some W the scores <img src="media/image80.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td>1.3</td>
<td>2.2</td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td><strong>4.9</strong></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td>2.0</td>
<td><strong>-3.1</strong></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Losses:</p>
</blockquote></td>
<td>2.9</td>
<td>0</td>
<td>10.9</td>
</tr>
</tbody>
</table>
<p><img src="media/image81.jpeg" style="width:10in;height:0.58889in" /></p>
<blockquote>
<p><strong>Multiclass SVM loss:</strong></p>
</blockquote>
<p><img src="media/image82.jpeg" style="height:4.85556in" /></p>
<p>Given an example</p>
<p>where the image and</p>
<p>where <img src="media/image83.jpeg" style="width:0.29028in;height:0.3125in" /> the (integer) label,</p>
<p>and using the shorthand for the scores vector: <img src="media/image84.jpeg" style="width:1.22986in;height:0.23958in" /></p>
<p>the SVM loss has the form:</p>
<p>Q5: usually at initialization W are small numbers, so all s ~= 0. What is the loss?</p>
<blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 17 11 Jan 2016</p>
<p><img src="media/image85.jpeg" style="width:5.9625in;height:0.53681in" /></p>
</blockquote>
<p><img src="media/image86.jpeg" style="width:10in;height:4.60208in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 18 11 Jan 2016</p>
<p><img src="media/image87.jpeg" style="width:9.60278in;height:0.58889in" /></p>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td><blockquote>
<p>Lecture 3 19</p>
</blockquote></td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p><img src="media/image90.jpeg" style="width:8.99167in;height:1.96111in" /></p>
</blockquote>
<p><img src="media/image91.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 20 11 Jan 2016</p>
<blockquote>
<p><img src="media/image92.jpeg" style="width:8.99167in;height:1.96111in" /></p>
</blockquote>
<p><img src="media/image93.jpeg" style="width:9.36736in;height:2.27014in" /></p>
<blockquote>
<p>E.g. Suppose that we found a W such that L = 0. Is this W unique?</p>
</blockquote>
<p><img src="media/image94.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 21 11 Jan 2016</p>
<blockquote>
<p><img src="media/image95.jpeg" style="width:6.13403in;height:4.86944in" /></p>
<p>With some W the scores <img src="media/image96.jpeg" style="width:1.6125in;height:0.3125in" /> are:</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>cat</p>
</blockquote></td>
<td><strong>3.2</strong></td>
<td></td>
<td>2.2</td>
</tr>
<tr class="even">
<td><blockquote>
<p>car</p>
</blockquote></td>
<td>5.1</td>
<td></td>
<td>2.5</td>
</tr>
<tr class="odd">
<td><blockquote>
<p>frog</p>
</blockquote></td>
<td>-1.7</td>
<td></td>
<td><strong>-3.1</strong></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Losses:</p>
</blockquote></td>
<td>2.9</td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image97.jpeg" style="width:10in;height:0.58889in" /></p>
<p><strong>Before:</strong></p>
<p><img src="media/image98.jpeg" style="height:4.85556in" /></p>
<ul>
<li><blockquote>
<p>max(0, 1.3 - 4.9 + 1) +max(0, 2.0 - 4.9 + 1)</p>
</blockquote></li>
<li><blockquote>
<p>max(0, -2.6) + max(0, -1.9)</p>
</blockquote></li>
<li><blockquote>
<p>0 + 0</p>
</blockquote></li>
<li><blockquote>
<p>0</p>
</blockquote></li>
</ul>
<p><img src="media/image100.jpeg" style="width:3.52014in;height:1.53056in" /></p>
<p><strong>With W twice as large:</strong></p>
<ul>
<li><blockquote>
<p>max(0, 2.6 - 9.8 + 1) +max(0, 4.0 - 9.8 + 1)</p>
</blockquote></li>
<li><blockquote>
<p>max(0, -6.2) + max(0, -4.8)</p>
</blockquote></li>
<li><blockquote>
<p>0 + 0</p>
</blockquote></li>
<li><blockquote>
<p>0</p>
</blockquote></li>
</ul>
<blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 22 11 Jan 2016</p>
<p>Weight Regularization</p>
</blockquote>
<p><img src="media/image101.jpeg" style="width:9.02778in;height:1.04722in" /></p>
<p>\lambda = regularization strength (hyperparameter)</p>
<p><img src="media/image102.jpeg" style="width:9.83056in;height:1.49236in" /></p>
<blockquote>
<p>In common use:</p>
<p><strong>L2 regularization</strong></p>
<p>L1 regularization <img src="media/image103.jpeg" style="width:2.81528in;height:0.42708in" /> Elastic net (L1 + L2) <img src="media/image104.jpeg" style="width:4.16319in;height:0.47917in" /> Max norm regularization (might see later) Dropout (will see later)</p>
</blockquote>
<p><img src="media/image105.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 23 11 Jan 2016</p>
<blockquote>
<p><img src="media/image106.jpeg" style="width:8.68819in;height:0.73403in" /></p>
</blockquote>
<p><img src="media/image107.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 24 11 Jan 2016</p>
<blockquote>
<p><img src="media/image111.jpeg" style="width:9.1125in;height:0.67222in" />(Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image112.jpeg" style="width:3.02153in;height:1.93056in" /></p>
<blockquote>
<p>cat</p>
</blockquote>
<p><img src="media/image113.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>car</p>
</blockquote>
<p><img src="media/image114.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>frog</p>
<p><strong>3.2</strong></p>
</blockquote>
<p><img src="media/image115.jpeg" style="width:1.35208in;height:0.41667in" /></p>
<blockquote>
<p>5.1</p>
</blockquote>
<p><img src="media/image116.jpeg" style="width:1.35208in;height:0.41667in" /></p>
<p>-1.7</p>
<p><img src="media/image117.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 25 11 Jan 2016</p>
<blockquote>
<p><img src="media/image118.jpeg" style="width:9.1125in;height:0.67222in" />(Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image119.jpeg" style="width:9.78264in;height:3.71597in" /></p>
<p><strong>scores = unnormalized log probabilities of the classes.</strong></p>
<blockquote>
<p>cat</p>
<p>car</p>
<p>frog</p>
<p><strong>3.2</strong></p>
<p>5.1</p>
</blockquote>
<p>-1.7</p>
<p><img src="media/image120.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 26 11 Jan 2016</p>
<blockquote>
<p><img src="media/image121.jpeg" style="width:9.1125in;height:0.67222in" />(Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image122.jpeg" style="width:9.78264in;height:3.71597in" /></p>
<p><strong>scores = unnormalized log probabilities of the classes.</strong></p>
<blockquote>
<p>where</p>
<p>cat</p>
<p>car</p>
<p>frog</p>
<p><strong>3.2</strong></p>
<p>5.1</p>
</blockquote>
<p>-1.7</p>
<p><img src="media/image123.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 27 11 Jan 2016</p>
<blockquote>
<p><img src="media/image124.jpeg" style="width:9.1125in;height:0.67222in" />(Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image125.jpeg" style="width:9.78264in;height:3.71597in" /></p>
<p><strong>scores = unnormalized log probabilities of the classes.</strong></p>
<blockquote>
<p>where</p>
<p>cat</p>
<p>car</p>
<p>frog</p>
<p><strong>3.2</strong></p>
<p>5.1</p>
</blockquote>
<p>-1.7</p>
<p>Softmax function</p>
<p><img src="media/image126.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 28 11 Jan 2016</p>
<blockquote>
<p><img src="media/image127.jpeg" style="width:9.1125in;height:0.67222in" />(Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image128.jpeg" style="width:9.78264in;height:3.71597in" /></p>
<blockquote>
<p>cat</p>
<p>car</p>
<p>frog</p>
</blockquote>
<p><strong>scores = unnormalized log probabilities of the classes.</strong></p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td><blockquote>
<p>where</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>3.2</strong></td>
<td><blockquote>
<p>Want to maximize the log likelihood, or (for a loss function)</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><blockquote>
<p>minimize the negative log likelihood of the correct class:</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>5.1</p>
</blockquote>
<p>-1.7</p>
<p><img src="media/image129.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 29 11 Jan 2016</p>
<blockquote>
<p><img src="media/image130.jpeg" style="width:9.1125in;height:0.67222in" />(Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image131.jpeg" style="width:9.78264in;height:3.71597in" /></p>
<blockquote>
<p>cat</p>
<p>car</p>
<p>frog</p>
</blockquote>
<p><strong>scores = unnormalized log probabilities of the classes.</strong></p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td><blockquote>
<p>where</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>3.2</strong></td>
<td><blockquote>
<p>Want to maximize the log likelihood, or (for a loss function)</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><blockquote>
<p>minimize the negative log likelihood of the correct class:</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>5.1</p>
</blockquote>
<p><sup>-1.7</sup> in summary:</p>
<p><img src="media/image132.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 30 11 Jan 2016</p>
<p><img src="media/image133.jpeg" style="width:9.1125in;height:0.67222in" /></p>
<blockquote>
<p><strong>Softmax Classifier</strong> (Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image134.jpeg" style="width:7.60278in;height:3.43889in" /></p>
<blockquote>
<p>cat</p>
</blockquote>
<p><img src="media/image135.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>car</p>
</blockquote>
<p><img src="media/image136.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>frog</p>
<p><strong>3.2</strong></p>
<p>5.1</p>
</blockquote>
<p>-1.7</p>
<p><img src="media/image137.jpeg" style="width:3.79306in;height:0.41667in" /></p>
<blockquote>
<p>unnormalized log probabilities</p>
</blockquote>
<p><img src="media/image138.jpeg" style="width:9.60278in;height:0.58889in" /></p>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td><blockquote>
<p>Lecture 3 31</p>
</blockquote></td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image139.jpeg" style="width:9.1125in;height:0.67222in" /></p>
<blockquote>
<p><strong>Softmax Classifier</strong> (Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image140.jpeg" style="width:6.20347in;height:1.44722in" /></p>
<blockquote>
<p>unnormalized probabilities</p>
</blockquote>
<p><img src="media/image141.jpeg" style="width:5.18819in;height:1.88125in" /></p>
<blockquote>
<p>cat</p>
</blockquote>
<p><img src="media/image142.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>car</p>
</blockquote>
<p><img src="media/image143.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>frog</p>
<p><strong>3.2</strong> <strong>24.5</strong></p>
<p>exp</p>
<p>5.1 <img src="media/image144.jpeg" style="width:0.82153in;height:0.15208in" /> 164.0</p>
</blockquote>
<p><img src="media/image145.jpeg" style="width:0.80972in" /></p>
<p>-1.7 0.18</p>
<p><img src="media/image146.jpeg" style="width:3.79306in;height:0.41667in" /></p>
<blockquote>
<p>unnormalized log probabilities</p>
</blockquote>
<p><img src="media/image147.jpeg" style="width:9.60278in;height:0.58889in" /></p>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td><blockquote>
<p>Lecture 3 32</p>
</blockquote></td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image148.jpeg" style="width:9.1125in;height:0.67222in" /></p>
<blockquote>
<p><strong>Softmax Classifier</strong> (Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image149.jpeg" style="width:6.20347in;height:1.44722in" /></p>
<blockquote>
<p>unnormalized probabilities</p>
</blockquote>
<p><img src="media/image150.jpeg" style="width:7.68819in;height:1.88125in" /></p>
<blockquote>
<p>cat</p>
</blockquote>
<p><img src="media/image151.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>car</p>
</blockquote>
<p><img src="media/image152.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>frog</p>
<p><strong>3.2</strong> <strong>24.5</strong> <strong>0.13</strong></p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td></td>
<td>exp</td>
<td>normalize</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>5.1</td>
<td></td>
<td>164.0</td>
<td></td>
<td>0.87</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image153.jpeg" style="width:0.80972in" /></p>
<p>-1.7 0.18 0.00</p>
<p><img src="media/image155.jpeg" style="width:7.24444in;height:0.41667in" /></p>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>unnormalized log probabilities</p>
</blockquote></td>
<td>probabilities</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td>Lecture 3 33</td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image156.jpeg" style="width:9.60278in;height:0.58889in" /></p>
<p><img src="media/image157.jpeg" style="width:9.1125in;height:0.67222in" /></p>
<blockquote>
<p><strong>Softmax Classifier</strong> (Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image158.jpeg" style="width:6.20347in;height:1.44722in" /></p>
<blockquote>
<p>unnormalized probabilities</p>
</blockquote>
<p><img src="media/image159.jpeg" style="width:9.87708in;height:1.88125in" /></p>
<blockquote>
<p>cat</p>
</blockquote>
<p><img src="media/image160.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>car</p>
</blockquote>
<p><img src="media/image161.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>frog</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><strong>3.2</strong></td>
<td><strong>24.5</strong></td>
<td><strong>0.13</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>exp</td>
<td>normalize</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5.1</td>
<td></td>
<td>164.0</td>
<td></td>
<td>0.87</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image162.jpeg" style="width:0.80972in" /></p>
<p>-1.7 0.18 0.00</p>
<p><img src="media/image164.jpeg" style="width:7.24444in;height:0.41667in" /></p>
<p>L_i = -log(0.13)</p>
<p><img src="media/image165.jpeg" style="width:0.24861in" /></p>
<ul>
<li><blockquote>
<p><strong>0.89</strong></p>
</blockquote></li>
</ul>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>unnormalized log probabilities</p>
</blockquote></td>
<td>probabilities</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td>Lecture 3 34</td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image166.jpeg" style="width:9.60278in;height:0.58889in" /></p>
<p><img src="media/image167.jpeg" style="width:9.1125in;height:0.67222in" /></p>
<blockquote>
<p><strong>Softmax Classifier</strong> (Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image168.jpeg" style="width:6.98819in;height:1.67014in" /></p>
<blockquote>
<p>Q: What is the min/max possible loss L_i?</p>
</blockquote>
<p><img src="media/image169.jpeg" style="width:1.275in;height:1.40417in" /></p>
<blockquote>
<p>unnormalized probabilities</p>
</blockquote>
<p><img src="media/image170.jpeg" style="width:9.87708in;height:1.88125in" /></p>
<blockquote>
<p>cat</p>
</blockquote>
<p><img src="media/image171.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>car</p>
</blockquote>
<p><img src="media/image172.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>frog</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><strong>3.2</strong></td>
<td><strong>24.5</strong></td>
<td><strong>0.13</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>exp</td>
<td>normalize</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5.1</td>
<td></td>
<td>164.0</td>
<td></td>
<td>0.87</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image173.jpeg" style="width:0.80972in" /></p>
<p>-1.7 0.18 0.00</p>
<p><img src="media/image175.jpeg" style="width:7.24444in;height:0.41667in" /></p>
<p>L_i = -log(0.13)</p>
<p><img src="media/image176.jpeg" style="width:0.24861in" /></p>
<ul>
<li><blockquote>
<p><strong>0.89</strong></p>
</blockquote></li>
</ul>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>unnormalized log probabilities</p>
</blockquote></td>
<td>probabilities</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td>Lecture 3 35</td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image177.jpeg" style="width:9.60278in;height:0.58889in" /></p>
<p><img src="media/image178.jpeg" style="width:9.1125in;height:0.67222in" /></p>
<blockquote>
<p><strong>Softmax Classifier</strong> (Multinomial Logistic Regression)</p>
</blockquote>
<p><img src="media/image179.jpeg" style="width:6.98819in;height:1.67014in" /></p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td><blockquote>
<p>Q5: usually at</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td><blockquote>
<p>initialization W are small</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>numbers, so all s ~= 0.</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>unnormalized probabilities</td>
<td><blockquote>
<p>What is the loss?</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image180.jpeg" style="width:9.87708in;height:3.39514in" /></p>
<blockquote>
<p>cat</p>
</blockquote>
<p><img src="media/image182.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>car</p>
</blockquote>
<p><img src="media/image183.jpeg" style="width:1.46528in;height:0.32569in" /></p>
<blockquote>
<p>frog</p>
</blockquote>
<table>
<tbody>
<tr class="odd">
<td><strong>3.2</strong></td>
<td><strong>24.5</strong></td>
<td><strong>0.13</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>exp</td>
<td>normalize</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5.1</td>
<td></td>
<td>164.0</td>
<td></td>
<td>0.87</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image184.jpeg" style="width:0.80972in" /></p>
<p>-1.7 0.18 0.00</p>
<p><img src="media/image186.jpeg" style="width:7.24444in;height:0.41667in" /></p>
<p>L_i = -log(0.13)</p>
<p><img src="media/image187.jpeg" style="width:0.24861in" /></p>
<ul>
<li><blockquote>
<p><strong>0.89</strong></p>
</blockquote></li>
</ul>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>unnormalized log probabilities</p>
</blockquote></td>
<td>probabilities</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td>Lecture 3 36</td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image188.jpeg" style="width:9.60278in;height:0.58889in" /></p>
<p><img src="media/image189.jpeg" style="width:9.78125in;height:5.59931in" /></p>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td><blockquote>
<p>Lecture 3 37</p>
</blockquote></td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p><img src="media/image190.jpeg" style="width:9.09028in;height:1.70764in" /></p>
</blockquote>
<p><img src="media/image191.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 38 11 Jan 2016</p>
<blockquote>
<p><img src="media/image192.jpeg" style="width:9.09028in;height:1.70764in" /></p>
</blockquote>
<p><img src="media/image193.jpeg" style="width:10in;height:2.83264in" /></p>
<blockquote>
<p>assume scores: [10, -2, 3] [10, 9, 9]</p>
<p>[10, -100, -100] and <img src="media/image194.jpeg" style="width:1.12153in;height:0.42708in" /></p>
</blockquote>
<ol start="17" type="A">
<li><p>Suppose I take a datapoint and I jiggle a bit (changing its score slightly). What happens to the loss in both cases?</p></li>
</ol>
<p><img src="media/image195.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 39 11 Jan 2016</p>
<blockquote>
<p><img src="media/image196.jpeg" style="width:8.7in;height:4.36667in" /></p>
</blockquote>
<p><img src="media/image197.jpeg" style="width:6.38681in;height:0.39097in" /></p>
<p><a href="http://vision.stanford.edu/teaching/cs231n/linear-classify-demo/"><span class="underline">http://vision.stanford.edu/teaching/cs231n/linear-classify-demo/</span></a></p>
<p><img src="media/image198.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 40 11 Jan 2016</p>
<p><img src="media/image199.jpeg" style="width:7.21389in;height:2.51042in" /></p>
<blockquote>
<p>Optimization</p>
</blockquote>
<p><img src="media/image200.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 41 11 Jan 2016</p>
<blockquote>
<p><img src="media/image201.jpeg" style="width:9.25972in;height:0.72569in" /></p>
</blockquote>
<p><img src="media/image202.jpeg" style="width:9.52361in;height:3.80208in" /></p>
<blockquote>
<p>- We have some dataset of (x,y) <sub>e.g.</sub></p>
</blockquote>
<ul>
<li><blockquote>
<p>We have a <strong>score function:</strong></p>
</blockquote></li>
<li><blockquote>
<p>We have a <strong>loss function</strong>:</p>
</blockquote></li>
</ul>
<blockquote>
<p>Softmax</p>
<p>SVM</p>
<p><img src="media/image203.jpeg" style="width:3.13333in;height:0.58542in" /> Full loss</p>
</blockquote>
<p><img src="media/image204.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 42 11 Jan 2016</p>
<blockquote>
<p><img src="media/image205.jpeg" style="width:9.06389in;height:0.80139in" /><strong>Random search</strong></p>
</blockquote>
<p><img src="media/image206.jpeg" style="width:10in;height:4.65833in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 43 11 Jan 2016</p>
<blockquote>
<p><img src="media/image207.jpeg" style="width:9.14861in;height:0.84306in" /></p>
</blockquote>
<p><img src="media/image208.jpeg" style="width:9.61389in;height:2.97153in" /></p>
<blockquote>
<p>15.5% accuracy! not bad!</p>
<p>(SOTA is ~95%)</p>
</blockquote>
<p><img src="media/image209.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 44 11 Jan 2016</p>
<p><img src="media/image210.jpeg" style="width:9.60278in;height:5.59653in" /></p>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td><blockquote>
<p>Lecture 3 45</p>
</blockquote></td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image211.jpeg" style="width:9.60278in;height:5.59653in" /></p>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td><blockquote>
<p>Lecture 3 46</p>
</blockquote></td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p><img src="media/image212.jpeg" style="width:9.06389in;height:0.80139in" /><strong>Follow the slope</strong></p>
</blockquote>
<p><img src="media/image213.jpeg" style="width:9.36042in;height:0.80139in" /></p>
<blockquote>
<p>In 1-dimension, the derivative of a function:</p>
</blockquote>
<p><img src="media/image214.jpeg" style="width:4.00694in;height:0.96528in" /></p>
<blockquote>
<p>In multiple dimensions, the <strong>gradient</strong> is the vector of (partial derivatives).</p>
</blockquote>
<p><img src="media/image216.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 47 11 Jan 2016</p>
<table>
<tbody>
<tr class="odd">
<td><strong>current W:</strong></td>
<td></td>
<td><blockquote>
<p><strong>gradient dW:</strong></p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>[0.34,</td>
<td></td>
<td><blockquote>
<p>[?,</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>-1.11,</td>
<td></td>
<td><blockquote>
<p>?,</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td>0.78,</td>
<td></td>
<td><blockquote>
<p>?,</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>0.12,</td>
<td></td>
<td><blockquote>
<p>?,</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td>0.55,</td>
<td></td>
<td><blockquote>
<p>?,</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>2.81,</td>
<td></td>
<td><blockquote>
<p>?,</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td>-3.1,</td>
<td></td>
<td><blockquote>
<p>?,</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td>-1.5,</td>
<td></td>
<td><blockquote>
<p>?,</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td>0.33,…]</td>
<td></td>
<td><blockquote>
<p>?,…]</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td><strong>loss 1.25347</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image217.jpeg" style="width:2.60347in;height:0.45833in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 48 11 Jan 2016</p>
<table>
<tbody>
<tr class="odd">
<td><strong>current W:</strong></td>
<td><blockquote>
<p><strong>W + h</strong> (first dim)<strong>:</strong></p>
</blockquote></td>
</tr>
<tr class="even">
<td>[0.34,</td>
<td><blockquote>
<p>[0.34 + <strong>0.0001</strong>,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.11,</td>
<td><blockquote>
<p>-1.11,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.78,</td>
<td><blockquote>
<p>0.78,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>0.12,</td>
<td><blockquote>
<p>0.12,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.55,</td>
<td><blockquote>
<p>0.55,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>2.81,</td>
<td><blockquote>
<p>2.81,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>-3.1,</td>
<td><blockquote>
<p>-3.1,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.5,</td>
<td><blockquote>
<p>-1.5,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.33,…]</td>
<td><blockquote>
<p>0.33,…]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><strong>loss 1.25347</strong></td>
<td><blockquote>
<p><strong>loss 1.25322</strong></p>
</blockquote></td>
</tr>
</tbody>
</table>
<p><img src="media/image222.jpeg" style="width:6.49028in;height:4.90694in" /></p>
<p><strong>gradient dW:</strong></p>
<p><img src="media/image225.jpeg" style="width:2.60347in;height:0.45833in" /></p>
<p>[?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,…]</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 49 11 Jan 2016</p>
<table>
<tbody>
<tr class="odd">
<td><strong>current W:</strong></td>
<td><blockquote>
<p><strong>W + h</strong> (first dim)<strong>:</strong></p>
</blockquote></td>
</tr>
<tr class="even">
<td>[0.34,</td>
<td><blockquote>
<p>[0.34 + <strong>0.0001</strong>,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.11,</td>
<td><blockquote>
<p>-1.11,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.78,</td>
<td><blockquote>
<p>0.78,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>0.12,</td>
<td><blockquote>
<p>0.12,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.55,</td>
<td><blockquote>
<p>0.55,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>2.81,</td>
<td><blockquote>
<p>2.81,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>-3.1,</td>
<td><blockquote>
<p>-3.1,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.5,</td>
<td><blockquote>
<p>-1.5,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.33,…]</td>
<td><blockquote>
<p>0.33,…]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><strong>loss 1.25347</strong></td>
<td><blockquote>
<p><strong>loss 1.25322</strong></p>
</blockquote></td>
</tr>
</tbody>
</table>
<p><img src="media/image226.jpeg" style="width:2.60347in;height:0.45833in" /></p>
<blockquote>
<p><strong>gradient dW:</strong></p>
</blockquote>
<p><img src="media/image230.jpeg" style="width:6.98472in;height:4.90694in" /></p>
<blockquote>
<p>[<strong>-2.5</strong>,</p>
<p>?,</p>
<p>?,</p>
</blockquote>
<p>(1.25322 - 1.25347)/0.0001 = -2.5</p>
<blockquote>
<p>?,</p>
<p>?,…]</p>
</blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 50 11 Jan 2016</p>
<table>
<tbody>
<tr class="odd">
<td><strong>current W:</strong></td>
<td><blockquote>
<p><strong>W + h</strong> (second dim)<strong>:</strong></p>
</blockquote></td>
</tr>
<tr class="even">
<td>[0.34,</td>
<td><blockquote>
<p>[0.34,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.11,</td>
<td><blockquote>
<p>-1.11 + <strong>0.0001</strong>,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.78,</td>
<td><blockquote>
<p>0.78,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>0.12,</td>
<td><blockquote>
<p>0.12,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.55,</td>
<td><blockquote>
<p>0.55,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>2.81,</td>
<td><blockquote>
<p>2.81,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>-3.1,</td>
<td><blockquote>
<p>-3.1,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.5,</td>
<td><blockquote>
<p>-1.5,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.33,…]</td>
<td><blockquote>
<p>0.33,…]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><strong>loss 1.25347</strong></td>
<td><blockquote>
<p><strong>loss 1.25353</strong></p>
</blockquote></td>
</tr>
</tbody>
</table>
<p><img src="media/image231.jpeg" style="height:4.80972in" /></p>
<p><strong>gradient dW:</strong></p>
<p><img src="media/image235.jpeg" style="width:2.60347in;height:0.45833in" /></p>
<p>[-2.5,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,…]</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 51 11 Jan 2016</p>
<table>
<tbody>
<tr class="odd">
<td><strong>current W:</strong></td>
<td><blockquote>
<p><strong>W + h</strong> (second dim)<strong>:</strong></p>
</blockquote></td>
</tr>
<tr class="even">
<td>[0.34,</td>
<td><blockquote>
<p>[0.34,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.11,</td>
<td><blockquote>
<p>-1.11 + <strong>0.0001</strong>,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.78,</td>
<td><blockquote>
<p>0.78,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>0.12,</td>
<td><blockquote>
<p>0.12,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.55,</td>
<td><blockquote>
<p>0.55,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>2.81,</td>
<td><blockquote>
<p>2.81,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>-3.1,</td>
<td><blockquote>
<p>-3.1,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.5,</td>
<td><blockquote>
<p>-1.5,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.33,…]</td>
<td><blockquote>
<p>0.33,…]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><strong>loss 1.25347</strong></td>
<td><blockquote>
<p><strong>loss 1.25353</strong></p>
</blockquote></td>
</tr>
</tbody>
</table>
<p><img src="media/image236.jpeg" style="height:4.80972in" /></p>
<blockquote>
<p><strong>gradient dW:</strong></p>
<p>[-2.5,</p>
<p><strong>0.6</strong>, <img src="media/image240.jpeg" /></p>
<p>?,</p>
<p>?,</p>
</blockquote>
<p>(1.25353 - 1.25347)/0.0001 = 0.6</p>
<blockquote>
<p>?,…]</p>
</blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 52 11 Jan 2016</p>
<table>
<tbody>
<tr class="odd">
<td><strong>current W:</strong></td>
<td><blockquote>
<p><strong>W + h</strong> (third dim)<strong>:</strong></p>
</blockquote></td>
</tr>
<tr class="even">
<td>[0.34,</td>
<td><blockquote>
<p>[0.34,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.11,</td>
<td><blockquote>
<p>-1.11,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.78,</td>
<td><blockquote>
<p>0.78 + <strong>0.0001</strong>,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>0.12,</td>
<td><blockquote>
<p>0.12,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.55,</td>
<td><blockquote>
<p>0.55,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>2.81,</td>
<td><blockquote>
<p>2.81,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>-3.1,</td>
<td><blockquote>
<p>-3.1,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.5,</td>
<td><blockquote>
<p>-1.5,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.33,…]</td>
<td><blockquote>
<p>0.33,…]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><strong>loss 1.25347</strong></td>
<td><blockquote>
<p><strong>loss 1.25347</strong></p>
</blockquote></td>
</tr>
</tbody>
</table>
<p><img src="media/image241.jpeg" style="height:4.80972in" /></p>
<p><strong>gradient dW:</strong></p>
<p><img src="media/image245.jpeg" style="width:2.60347in;height:0.45833in" /></p>
<p>[-2.5,</p>
<p>0.6,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,</p>
<p>?,…]</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 53 11 Jan 2016</p>
<table>
<tbody>
<tr class="odd">
<td><strong>current W:</strong></td>
<td><blockquote>
<p><strong>W + h</strong> (third dim)<strong>:</strong></p>
</blockquote></td>
</tr>
<tr class="even">
<td>[0.34,</td>
<td><blockquote>
<p>[0.34,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.11,</td>
<td><blockquote>
<p>-1.11,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.78,</td>
<td><blockquote>
<p>0.78 + <strong>0.0001</strong>,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>0.12,</td>
<td><blockquote>
<p>0.12,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.55,</td>
<td><blockquote>
<p>0.55,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>2.81,</td>
<td><blockquote>
<p>2.81,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>-3.1,</td>
<td><blockquote>
<p>-3.1,</p>
</blockquote></td>
</tr>
<tr class="odd">
<td>-1.5,</td>
<td><blockquote>
<p>-1.5,</p>
</blockquote></td>
</tr>
<tr class="even">
<td>0.33,…]</td>
<td><blockquote>
<p>0.33,…]</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><strong>loss 1.25347</strong></td>
<td><blockquote>
<p><strong>loss 1.25347</strong></p>
</blockquote></td>
</tr>
</tbody>
</table>
<p><img src="media/image246.jpeg" style="height:4.80972in" /></p>
<blockquote>
<p><strong>gradient dW:</strong></p>
</blockquote>
<p><img src="media/image249.jpeg" style="height:4.84097in" /></p>
<blockquote>
<p>[-2.5,</p>
<p>0.6,</p>
<p><strong>0</strong>, <img src="media/image250.jpeg" /></p>
<p>?,</p>
</blockquote>
<p>(1.25347 - 1.25347)/0.0001 = 0</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 54 11 Jan 2016</p>
<p><img src="media/image251.jpeg" style="width:9.80833in;height:5.00278in" /></p>
<p><img src="media/image252.jpeg" style="width:10in;height:0.58889in" /></p>
<blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 55 11 Jan 2016</p>
</blockquote>
<p><img src="media/image253.jpeg" style="width:9.80833in;height:5.00278in" /></p>
<ul>
<li><blockquote>
<p>approximate</p>
</blockquote></li>
<li><blockquote>
<p>very slow to evaluate</p>
</blockquote></li>
</ul>
<p><img src="media/image254.jpeg" style="width:10in;height:0.58889in" /></p>
<blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 56 11 Jan 2016</p>
<p><img src="media/image255.jpeg" style="width:9.17222in;height:4.62847in" /></p>
<p>want <img src="media/image256.jpeg" style="width:0.97639in;height:0.45833in" /></p>
</blockquote>
<p><img src="media/image257.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 57 11 Jan 2016</p>
<blockquote>
<p><img src="media/image258.jpeg" style="width:9.37292in;height:4.76597in" /></p>
<p>want <img src="media/image259.jpeg" style="width:0.97639in;height:0.45833in" /></p>
</blockquote>
<p><img src="media/image260.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 58 11 Jan 2016</p>
<blockquote>
<p><img src="media/image261.jpeg" style="width:9.46944in;height:4.75278in" /></p>
<p>want <img src="media/image262.jpeg" style="width:0.97639in;height:0.45833in" /></p>
<p>Calculus</p>
</blockquote>
<p><img src="media/image263.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 59 11 Jan 2016</p>
<blockquote>
<p><img src="media/image264.jpeg" style="width:9.10486in;height:4.62847in" /></p>
<p><img src="media/image265.jpeg" style="width:0.97639in;height:0.45833in" /> = ...</p>
</blockquote>
<p><img src="media/image266.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 60 11 Jan 2016</p>
<blockquote>
<p><img src="media/image267.jpeg" style="height:4.80972in" /></p>
<p>[0.34,</p>
<p>-1.11,</p>
<p>0.78,</p>
<p>0.12,</p>
<p>0.55,</p>
<p>2.81,</p>
<p>-3.1,</p>
<p>-1.5,</p>
<p>0.33,…] <strong>loss 1.25347</strong></p>
</blockquote>
<p>dW = ...</p>
<p><img src="media/image269.jpeg" style="width:3.77847in;height:3.41042in" /></p>
<p>(some function</p>
<p>data and W)</p>
<p><img src="media/image270.jpeg" style="width:10in;height:0.58889in" /></p>
<p><strong>gradient dW:</strong></p>
<p><img src="media/image271.jpeg" style="width:2.60347in;height:0.45833in" /></p>
<p>[-2.5,</p>
<p>0.6,</p>
<p>0,</p>
<p>0.2,</p>
<p>0.7,</p>
<p>-0.5,</p>
<p>1.1,</p>
<p>1.3,</p>
<p>-2.1,…]</p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 61 11 Jan 2016</p>
<blockquote>
<p><img src="media/image272.jpeg" style="width:8.98403in;height:4.19306in" /></p>
</blockquote>
<ul>
<li><blockquote>
<p>Numerical gradient: approximate, slow, easy to write</p>
</blockquote></li>
<li><blockquote>
<p>Analytic gradient: exact, fast, error-prone</p>
</blockquote></li>
</ul>
<blockquote>
<p>=&gt;</p>
<p><span class="underline">In practice:</span> Always use analytic gradient, but check implementation with numerical gradient. This is called a <strong>gradient check.</strong></p>
</blockquote>
<p><img src="media/image273.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 62 11 Jan 2016</p>
<blockquote>
<p><img src="media/image274.jpeg" style="width:7.36944in;height:1.23542in" /></p>
</blockquote>
<p><img src="media/image275.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 63 11 Jan 2016</p>
<blockquote>
<p><img src="media/image277.jpeg" style="width:7.29167in;height:4.3125in" /></p>
</blockquote>
<p><img src="media/image278.jpeg" style="width:2.10417in;height:0.45833in" /></p>
<p>original W</p>
<p><img src="media/image280.jpeg" style="width:1.89653in" /></p>
<blockquote>
<p>W_1</p>
<p>negative gradient direction</p>
</blockquote>
<p><img src="media/image282.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 64 11 Jan 2016</p>
<blockquote>
<p><img src="media/image283.jpeg" style="width:8.82222in;height:0.76944in" /></p>
</blockquote>
<p><img src="media/image284.jpeg" style="width:8.72778in;height:0.62569in" /></p>
<ul>
<li><blockquote>
<p>only use a small portion of the training set to compute the gradient.</p>
</blockquote></li>
</ul>
<p><img src="media/image285.jpeg" style="width:9.35069in;height:2.85208in" /></p>
<blockquote>
<p>Common mini-batch sizes are 32/64/128 examples e.g. Krizhevsky ILSVRC ConvNet used 256 examples</p>
</blockquote>
<p><img src="media/image286.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 65 11 Jan 2016</p>
<blockquote>
<p><img src="media/image287.jpeg" style="width:9.64792in;height:3.88056in" /></p>
<p>(Loss over mini-batches goes down over time.)</p>
</blockquote>
<p><img src="media/image288.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 66 11 Jan 2016</p>
<blockquote>
<p><img src="media/image289.jpeg" style="width:4.95278in;height:4.64514in" /></p>
</blockquote>
<p><img src="media/image290.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 67 11 Jan 2016</p>
<p><img src="media/image292.jpeg" style="width:8.82222in;height:0.76944in" /></p>
<blockquote>
<p>Mini-batch Gradient Descent</p>
</blockquote>
<p><img src="media/image293.jpeg" style="width:8.72778in;height:0.62569in" /></p>
<ul>
<li><blockquote>
<p>only use a small portion of the training set to compute the gradient.</p>
</blockquote></li>
</ul>
<p><img src="media/image294.jpeg" style="width:9.72708in;height:3.16389in" /></p>
<blockquote>
<p>Common mini-batch sizes are 32/64/128 examples e.g. Krizhevsky ILSVRC ConvNet used 256 examples</p>
</blockquote>
<p><img src="media/image295.jpeg" style="width:9.60278in;height:0.58889in" /></p>
<p>we will look at more fancy update formulas (momentum, Adagrad, RMSProp, Adam, …)</p>
<table>
<tbody>
<tr class="odd">
<td><blockquote>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>
</blockquote></td>
<td><blockquote>
<p>Lecture 3 68</p>
</blockquote></td>
<td><blockquote>
<p>11 Jan 2016</p>
</blockquote></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p><img src="media/image296.jpeg" style="width:9.81042in;height:4.71667in" /></p>
</blockquote>
<p>(image credits to Alec Radford)</p>
<p><img src="media/image297.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 69 11 Jan 2016</p>
<blockquote>
<p><img src="media/image298.jpeg" style="width:9.54097in;height:0.63611in" /></p>
</blockquote>
<p><img src="media/image299.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 70 11 Jan 2016</p>
<blockquote>
<p><img src="media/image301.jpeg" style="width:6.27986in;height:0.87222in" /></p>
</blockquote>
<p><img src="media/image302.jpeg" style="height:2.82708in" /></p>
<blockquote>
<p>hue bins</p>
<p><img src="media/image314.jpeg" style="width:0.15208in;height:0.17917in" /> +1</p>
</blockquote>
<p><img src="media/image315.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 71 11 Jan 2016</p>
<blockquote>
<p><img src="media/image316.jpeg" style="width:6.27986in;height:0.87222in" /></p>
</blockquote>
<p><img src="media/image317.jpeg" style="width:2.20278in;height:0.56875in" /></p>
<blockquote>
<p>8x8 pixel region, quantize the edge orientation into 9 bins</p>
</blockquote>
<p><img src="media/image318.jpeg" style="width:9.91667in;height:3.79236in" /></p>
<p><em>(image from vlfeat.org)</em></p>
<p><img src="media/image320.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 72 11 Jan 2016</p>
<blockquote>
<p><img src="media/image321.jpeg" style="width:6.27986in;height:0.87222in" /></p>
</blockquote>
<p><img src="media/image322.jpeg" style="width:6.30486in;height:3.52083in" /></p>
<blockquote>
<p>8x8 pixel region, quantize the edge orientation into 9 bins</p>
</blockquote>
<p><img src="media/image323.jpeg" style="width:3.36111in;height:3.36111in" /></p>
<blockquote>
<p>Many more:</p>
<p>GIST, LBP, Texton, SSIM, ...</p>
</blockquote>
<p><img src="media/image324.jpeg" style="width:2.42917in;height:0.36042in" /></p>
<p><em>(image from vlfeat.org)</em></p>
<p><img src="media/image325.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 73 11 Jan 2016</p>
<table>
<tbody>
<tr class="odd">
<td>Example: Bag of Words</td>
<td><blockquote>
<p>histogram of</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><blockquote>
<p>visual words</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><blockquote>
<p>visual word vectors</p>
</blockquote></td>
<td></td>
<td><blockquote>
<p>learn k-means centroids</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><blockquote>
<p>“vocabulary” of visual words</p>
</blockquote></td>
<td><blockquote>
<p>1000-d vector</p>
</blockquote></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image326.jpeg" style="width:6.27986in;height:0.66667in" /></p>
<table>
<tbody>
<tr class="odd">
<td>144</td>
<td></td>
<td><blockquote>
<p>1000-d vector</p>
</blockquote></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image329.jpeg" style="width:1.18611in" /></p>
<blockquote>
<p>1000-d vector</p>
</blockquote>
<p><img src="media/image339.jpeg" style="width:1.18611in" /></p>
<blockquote>
<p>e.g. 1000 centroids</p>
</blockquote>
<p><img src="media/image347.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 74 11 Jan 2016</p>
<blockquote>
<p><img src="media/image349.jpeg" style="width:2.64514in;height:0.28889in" /></p>
<p>image statistics</p>
</blockquote>
<p><img src="media/image350.jpeg" style="height:0.47847in" /></p>
<table>
<tbody>
<tr class="odd">
<td>Feature</td>
<td></td>
<td></td>
<td><blockquote>
<p>f</p>
</blockquote></td>
<td><blockquote>
<p><strong>10</strong> numbers, indicating</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Extraction</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><blockquote>
<p>class scores</p>
</blockquote></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td><blockquote>
<p>training</p>
</blockquote></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="media/image352.jpeg" style="width:0.72361in" /></p>
<blockquote>
<p><strong>[32x32x3]</strong></p>
<p>f</p>
<p><strong>10</strong> numbers, indicating class scores</p>
</blockquote>
<p><img src="media/image356.jpeg" style="width:4.06111in" /></p>
<blockquote>
<p>training</p>
<p><strong>[32x32x3]</strong></p>
</blockquote>
<p><img src="media/image358.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 75 11 Jan 2016</p>
<p><img src="media/image359.jpeg" style="width:9.51597in;height:1.35139in" /></p>
<p>Becoming a backprop ninja</p>
<p>and</p>
<p>Neural Networks (part 1)</p>
<p><img src="media/image360.jpeg" style="width:10in;height:0.58889in" /></p>
<p>Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson Lecture 3 76 11 Jan 2016</p>
